# biggie

Biggie is a tool to
- quickly get data from (external) APIs into either a Mongo or Postgres database
- have it exposed/searchable via a dedicated API
- investigate it from a Jupyter Lab server

It is a Docker compose setup including
- an orchestrator container based on Celery (including Beat) and asyncio
- A Spark cluster made of a master and 2 workers containers
- an API endpoints container based on FastAPI
- 2 containers for MongoDB and PostgreSQL databases
- 3 containers for Flower, Mongo-Express and DBeaver for monitoring purpose
- a Jupyter Lab container to play around with all this

It is currently set up to fetch data from the [GitHub Events API](https://api.github.com/events)
then stream it into Mongo and have it exposed/analysed via several dedicated APIs.
It was fetching data from the Marvel API until version 0.4.0.

The repository itself is based on the ['Papel' repository](https://github.com/pierrz/papel).

---

### Table of Contents
- [Installation](#installation)
  - [Environment](#environment)
  - [Test](#test)
- [Run](#run)
  - [Data acquisition](#data-acquisition)
  - [Data acquisition with monitoring](#data-acquisition-with-monitoring)
  - [API container with monitoring](#api-container-with-monitoring)
  - [Acquisition, analytics/API and Jupyter containers with monitoring](#acquisition-analyticsapi-and-jupyter-containers-with-monitoring)
- [Teleport and Terraform](#teleport-and-terraform)
- [Nginx deployment](#nginx-deployment-only-exposing-the-api-jupyter-and-monitoring-containers)
- [Local URLs](#local-urls)
- [Development](#development)
- [Contribute](#contribute)

<br>

---


### Installation

#### Environment
You have to create the `.env` environment file and use/create a Github token for enabling the test buid sequence.

[wip] Eventually tweak the schedule parameter for the cleaning task (see **"Data streaming"** section below.).

If you plan to use the same Github-actions CI file, you need to create the same secrets
as in the `jobs > env` section in `.github/workflows/docker-ci.yaml` (see **line 31**).

**NB**:
- For all files embedded with secrets, you'll find the `<file>.example` ready to adapt.

<br>

#### Test
```
docker compose up api-test orchestrator-test
OR
docker compose --profile test up
```

<br>

### Run
#### Data acquisition
```
docker compose up orchestrator-prod
```
This command will spin up the Orchestrator container and:

  - download all required data and save them as files locally
  - read these files and load Postgres with relevant data
  - delete all local files once their data is successfully in Mongo

These tasks are **scheduled every minute** with a crontab setting,
and a custom parameter is implemented to separately schedule the cleaning step
while keeping it in sync with the rest of the chain.

See `kwargs={"wait_minutes": 30}` in the `github_events_stream` schedule in [**`.../tasks/schedules.py`**](orchestrator/src/tasks/schedules.py#L25).

```
docker compose \
  -f compose.yaml \
  --profile prod_acquisition \
  up
```

<br>

#### Data acquisition with monitoring
You can just pass the monitoring configuration to include the pending containers
with any profile or container command:
```-f compose.monitoring.yaml```

For Data acquisition, this will spin up both the Mongo-Express and Flower containers
along the production containers.
```
docker compose -f compose.yaml -f compose.monitoring.yaml --profile monitoring up
docker compose \
  -f compose.yaml \
  -f compose.monitoring.yaml \
  --profile prod_acquisition \
  up
```

<br>

#### API container with monitoring
Just to have the FastAPI container up
```
docker compose \
  -f compose.yaml \
  -f compose.monitoring.yaml \
  --profile prod_analytics \
  up
```

<br>

#### Acquisition, analytics/API and Jupyter containers with monitoring
Spin up the whole shewbang with:
```
docker compose \
  -f compose.yaml \
  -f compose.monitoring.yaml \
  --profile prod_full \
  up
```

<br>

### Teleport and Terraform
The repository includes a [Terraform configuration](/terraform/main.tf), currently tuned to work with [Scaleway](https://registry.terraform.io/providers/scaleway/scaleway/latest/docs).
It prepares and checks the plan, then connects to the server via [Teleport](https://goteleport.com/docs/admin-guides/deploy-a-cluster/linux-demo/) (required to be installed and set up on the host machine) and deploy the repository with the secrets stored in Github Actions. This is all handled via the [**CD pipeline**](.github/workflows/cd.yml).

<br>

### Nginx deployment (only exposing the API, Jupyter and monitoring containers)
In this configuration, you need to have the necessary sub-domains setup on your domain provider side.
You also need:
- Nginx installed on the host machine
- a certificate generated by `certbot` without any changes to nginx configuration (see [documentation](https://certbot.eff.org/instructions))
    ```
    sudo certbot certonly --nginx    # example command for ubuntu 24
    ```
<br>

Then create the required files and change the `volumes` path accordingly in the `compose` files.
The `nginx` configuration files are:

`nginx/certificate.conf`
`nginx/containers.conf`

<br>

### Local URLs

[API docs](http://localhost:8000/docs)

- [List of PR events count per repo; default limit at 50 repos](http://localhost:8000/events/counts?limit=10)
- [PR events count for a given repo](http://localhost:8000/events/count?repo_name=<repository-name>)
- [Events total counts for all gathered repos with offset in minutes from now; default offset is 0 minute](http://localhost:8000/events/count_per_type/all?offset=<int>)
- [Events total counts for a given repo with offset; same default as above](http://localhost:8000/events/count_per_type?repo_name=<repository-name>&offset=<int>)
- [PR average delta for a given repository](http://localhost:8000/events/pr_average_delta?repo_name=<repository-name>)
- [Timeline of PR deltas for a given repository (dataviz); default size is 3](http://localhost:8000/events/pr_deltas_timeline?repo_name=<repository-name>&size=<int>)
- [Dashboard (list of counts for 50 most active repos)](http://localhost:8000/events/dashboard)
- [Detailed UI for a given repo, based on several endpoints and dataviz above.](http://localhost:8000/events/details?repo_name=<repository-name>)

NB: The repository name parameter takes the full repository name
including the actor name such as `pierrz/biggie`.

Monitoring
- [Mongo-Express](http://localhost:8081)
- [Flower](http://localhost:49555)
- [DBeaver](http://localhost:8978)

<br>

### Development
If you want to make some changes in this repo while following the same environment tooling,
you can run the following command from the root directory:
```
poetry config virtualenvs.in-project true
poetry install && poetry shell
pre-commit install
```

To change the code of the core containers, you need to `cd` to the related directory
and either:
- run `poetry update` to simply install the required dependencies
- run the previous command to create a dedicated virtualenv

### Contribute
You can always propose a PR based on a `TODO`, just don't forget to update the release version
that you can find in [`ci.yaml`](.github/workflows/ci.yml#L18) and all `pyproject.toml` files.
