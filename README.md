# biggie

Biggie is a tool to quickly get data from (external) APIs into either a Mongo or Postgres database,
and have it exposed/searchable via a dedicated new API.

It is a Docker compose setup including
- a "worker" container based on Celery (including Beat), Spark and asyncio
- an API endpoints container based on FastAPI
- 2 containers for Mongo and Postgres databases.
- 2 containers for Flower and PGAdmin for monitoring purpose.
- a 'live deployment' container for Nginx and currently set up only with the Api container

It is currently set up to fetch data from the [GitHub Events API](https://api.github.com/events)
then stream it into Mongo and have it exposed/analysed via several dedicated APIs.
It was fetching data from the Marvel API until version 0.4.0.

The repository itself is based on the ['Papel' repository](https://github.com/pierrz/papel).

<br>


### Installation

#### Environment
You have to create the `.env` environment file and use/create a Github token for enabling the test buid sequence.

[wip] Eventually tweak the schedule parameter for the cleaning task (see **"Data streaming"** section below.).

If you plan to use the same Github-actions CI file, you need to create the same secrets
as in the `jobs > env` section in `.github/workflows/docker-ci.yml` (see **line 31**).

**NB**:
- For all files embedded with secrets, you'll find the `<file>.example` ready to adapt.

<br>

#### Test
```
docker compose up api_test celery_test
OR
docker compose --profile test up
```

<br>

### Run
#### Data acquisition
```
docker compose up celery_prod
```
This command will spin up the Celery container and:

  - download all required data and save them as files locally
  - read these files and load Postgres with relevant data
  - delete all local files once their data is successfully in Mongo

These tasks are **scheduled every minute** with a crontab setting,
and a custom parameter is implemented to separately schedule the cleaning step
while keeping it in sync with the rest of the chain.

See `kwargs={"wait_minutes": 30}` in the `github_events_stream` schedule in [**`.../tasks/schedules.py`**](celery_app/src/tasks/schedules.py).

```
docker compose -f docker-compose.yml -f docker-compose.monitoring.yml --profile monitoring up
docker-compose \
  -f docker-compose.yml \
  --profile prod_acquisition \
  up
```

<br>

#### Data acquisition with monitoring
You can just pass the monitoring configuration to include the pending containers
with any profile or container command:
```-f docker-compose.monitoring.yml```

For Data acquisition, this will spin up both the Mongo-Express and Flower containers
along the production containers.
```
docker compose -f docker-compose.yml -f docker-compose.monitoring.yml --profile monitoring up
docker-compose \
  -f docker-compose.yml \
  -f docker-compose.monitoring.yml \
  --profile prod_acquisition \
  up
```

<br>

#### API container with monitoring
Just to have the FastAPI container up
```
docker compose \
  -f docker-compose.yml \
  -f docker-compose.monitoring.yml \
  --profile prod_analytics \
  up
```

<br>

#### Acquisiton and Analytics/API Production containers
Both production containers as well as both monitoring containers.
```
docker compose \
  -f docker-compose.yml \
  -f docker-compose.monitoring.yml \
  --profile prod_full \
  up
```

<br>

### Nginx deployment (only exposing the API container)
In this configuration, you need to have the necessary sub-domains setup on your domain provider side.
You also need:
- Nginx installed on the host machine
- a certificate generated by `certbot` without any changes to nginx configuration (see [documentation](https://certbot.eff.org/instructions))
    ```
    sudo certbot certonly --nginx    # example command for ubuntu 20
    ```
<br>

Then create the required files and change the `volumes` path accordingly in the `compose` files.
The `nginx` configuration files are:

`conf/nginx/certificate.json`
`conf/nginx/app_docker.conf`
`conf/nginx/monitor_docker.conf`
<br>

Finally run the `docker-compose` command with the `live_prod` profile
to spin up all that to the world:
```
docker compose \
  -f docker-compose.yml \
  -f docker-compose.monitoring.yml \
  --profile prod_full --profile live_prod \
  up
```

<br>

### Local URLs

[API docs](http://localhost:8000/docs)

- [List of PR events count per repo; default limit at 50 repos](http://localhost:8000/events/counts?limit=10)
- [PR events count for a given repo](http://localhost:8000/events/count?repo_name=<repository-name>)
- [Events total counts for all gathered repos with offset in minutes from now; default offset is 0 minute](http://localhost:8000/events/count_per_type/all?offset=<int>)
- [Events total counts for a given repo with offset; same default as above](http://localhost:8000/events/count_per_type?repo_name=<repository-name>&offset=<int>)
- [PR average delta for a given repository](http://localhost:8000/events/pr_average_delta?repo_name=<repository-name>)
- [Timeline of PR deltas for a given repository (dataviz); default size is 3](http://localhost:8000/events/pr_deltas_timeline?repo_name=<repository-name>&size=<int>)
- [Dashboard (list of counts for 50 most active repos)](http://localhost:8000/events/dashboard)
- [Detailed UI for a given repo, based on several endpoints and dataviz above.](http://localhost:8000/events/details?repo_name=<repository-name>)

NB: The repository name parameter takes the full repository name
including the actor name such as `pierrz/biggie`.

Monitoring
- [Mongo-Express](http://localhost:8081)
- [Flower](http://localhost:49555)
- [PGAdmin](http://localhost:5050)

<br>

### Local development
If you want to make some changes in this repo while following the same environment tooling,
you can run the following command from the root directory:
```
poetry config virtualenvs.in-project true
poetry install && poetry shell
pre-commit install
```

To change the code of the core containers, you need to `cd` to the related directory
and either:
- run `poetry update` to simply install the required dependencies
- run the previous command to create a dedicated virtualenv

### Contribute
You can always propose a PR, just don't forget to update the release version
that you can find in `ci.yml` and all `pyproject.toml` files.
