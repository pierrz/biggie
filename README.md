# biggie

Biggie is a tool to quickly get data from (external) APIs into a Mongo database,
and have it exposed/searchable via a dedicated new API.

It is a Docker compose setup including
- a "worker" container based on Celery (including Beat), Spark and asyncio
- an API endpoints container based on FastAPI
- 2 containers for Mongo and Postgres databases.
- 2 containers for Flower and PGAdmin for monitoring purpose.
- a 'live deployment' container for Nginx and currently set up only with the Api container

It is currently set up to fetch data from the [GitHub Events API](https://api.github.com/events)
then stream it into Mongo and have it exposed/analysed via several dedicated APIs. 
It was fetching data from the Marvel API until version 0.4.0. 

The repository itself is based on the ['Papel' repository](https://github.com/pierrz/papel).

<br>


### Installation

#### Environment
You have to create the `.env` environment file and use/create a Github token for enabling the test buid sequence.

[wip] Eventually tweak the schedule parameter for the cleaning task (see **"Data streaming"** section below.).

If you plan to use the same Github-actions CI file, you need to create the same secrets
as in the `jobs > env` section in `.github/workflows/docker-ci.yml` (see **line 31**).

**NB**:
- For all files embedded with secrets, you'll find the `<file>.example` ready to adapt.

<br>

#### Build
The `docker-compose.main` file is structured to make the `test` containers build the image
used by the `prod` image. Hence the need to run one of the following commands on the very first run:
```
docker-compose up api_test celery_test
OR
docker-compose --profile test up
```

<br>

### Run
#### Data acquisition
```
docker-compose up celery_prod
```
This command will spin up the Celery container and:

  - download all required data and save them as files locally
  - read these files and load Postgres with relevant data
  - [tdb] delete all local files once their data is successfully in Mongo

These tasks are **scheduled every minute** with a crontab setting,
and a custom parameter is implemented to separately schedule the cleaning step
while keeping it in sync with the rest of the chain.

See `kwargs={"wait_minutes": 30}` in the `beat_schedule` parameter in [**`celery_app/config.py`**](celery_app/config.py).

<br>

#### Data acquisition with monitoring
Spin up the Mongo-Express container to access the Mongo-Express and Flower UI
along the Celery production container.
```
docker-compose -f docker-compose.yml -f docker-compose.monitoring.yml --profile monitoring up
docker-compose \
  -f docker-compose.yml \
  -f docker-compose.monitoring.yml \
  --profile monitoring \
  up
```

<br>

#### API container
Just to have the FastAPI container up
```
docker-compose up api_prod
```

<br>

#### Monitoring and Production containers
Both production containers as well as both monitoring containers.
```
docker-compose \
  -f docker-compose.yml \
  -f docker-compose.monitoring.yml \
  --profile prod --profile monitoring \
  up
```

<br>

### Nginx deployment (API container only)
In this configuration, you need to have the necessary sub-domains setup on your domain provider side.
You also need:
- Nginx installed on the host machine
- a certificate generated by `certbot` without any changes to nginx configuration (see [documentation](https://certbot.eff.org/instructions))
    ```
    sudo certbot certonly --nginx    # example command for ubuntu 20
    ```
<br>

Then create the required files and change the `volumes` path accordingly in the `compose` files.
The `nginx` configuration files are:

`conf/nginx/certificate.json`
`conf/nginx/app_docker.conf`
`conf/nginx/monitor_docker.conf`
<br>

Finally run the `docker-compose` command with the `live_prod` profile
to spin up all that to the world:
```
docker-compose \
  -f docker-compose.yml \
  -f docker-compose.monitoring.yml \
  --profile prod --profile monitoring --profile live_prod \
  up
```

<br>

### Local URLs

[API docs](http://localhost:8000/docs)

Github events consumer
- [Count per type with a given time offset in minutes](http://localhost:8000/api/count_per_type?offset=90)
- [PR average delta for a given repository](http://localhost:8000/api/pr_average_delta?repo_name=<repository-name>)
- [Timeline of PR deltas for a given repository (dataviz)](http://localhost:8000/api/pr_deltas_timeline?repo_name=<repository-name>)

NB: For the last 2 endpoints, the repository name parameter takes the full repository name
including the actor name such as `pierrz/biggie`.

Monitoring
- [Mongo-Express](http://localhost:8081)
- [Flower](http://localhost:49555)

<br>

### Local development
If you want to make some changes in this repo while following the same environment tooling,
you can run the following command from the root directory:
```
poetry config virtualenvs.in-project true
poetry install && poetry shell
pre-commit install
```

To change the code of the core containers, you need to `cd` to the related directory
and either:
- run `poetry update` to simply install the required dependencies
- run the previous command to create a dedicated virtualenv
